# File: tokenizer.py

def tokenize_text(text):
    # Simple tokenizer that splits text into words
    return text.split()

